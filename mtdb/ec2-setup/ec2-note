Running on EC2 VM

Set hostname
	Dot in the hostname doesn't show up in the prompt, e.g.,
	hostname0=ec2-m3.xlarge.s0, but it is there when you run hostname.

	Excute both for export ec2-s0-m3-xlarge ec2-c0-m3-xlarge

	TODO: can be set from my machine, using ssh -c or something. when time allows.

	export hostname0=ec2-s0-c3.2xlarge-26 \
	&& sudo sed -i 's/localhost$/localhost '$hostname0'/g' /etc/hosts \
	&& sudo bash -c 'echo '$hostname0' > /etc/hostname' \
	&& sudo hostname --file /etc/hostname

	export hostname0=ec2-c0-m4.large-26 \
	&& sudo sed -i 's/localhost$/localhost '$hostname0'/g' /etc/hosts \
	&& sudo bash -c 'echo '$hostname0' > /etc/hostname' \
	&& sudo hostname --file /etc/hostname

	exit and login back


Install packages, vmtouch
, one-time synchronization of clocks both on server and client
, setup repositories and dev envs

	(will be prompted for an Oracle binary code license)
	(will be prompted for if you trust the server)

	rsync from an existing machine, when the ubuntu servers are slow.
		(rsync -av ubuntu@23.22.13.183:/var/cache/apt/archives ~/ || true) \
		&& (sudo cp ~/archives/* /var/cache/apt/archives/ || true) \

	sudo add-apt-repository -y ppa:webupd8team/java \
	&& sudo apt-get update \
	&& sudo apt-get install oracle-java8-installer git ctags ant htop tree maven \
	gnuplot-nox ntp ioping realpath make gcc cmake g++ \
	libboost-dev libboost-system-dev libboost-timer-dev \
	collectl -y \
	&& sudo apt-get autoremove -y vim-tiny \
	&& mkdir -p ~/work \
	&& cd ~/work \
	&& git clone https://github.com/hoytech/vmtouch.git \
	&& cd vmtouch \
	&& make -j \
	&& sudo make install \
	&& sudo service ntp stop \
	&& sudo ntpdate -bv 0.ubuntu.pool.ntp.org \
	&& sudo service ntp start \
	&& cd ~/work \
	&& git clone git@github.com:hobinyoon/linux-home.git \
	&& cd linux-home \
	&& ./setup-linux.sh

	exit and login back


Set firewall rules on Server. Allow client machine to connect to server machine
on AWS console
	Allow inbound TCP 9042 10.152.33.227/32


Set server address on client machine
	TODO: this can be automated from a control machine.

	On server:
		echo 'export CASSANDRA_CLIENT_ADDR=54.88.197.238' >> ~/.bashrc
		echo 'export CASSANDRA_CLIENT_ADDR=52.23.248.65'  >> ~/.bashrc
		echo 'export CASSANDRA_CLIENT_ADDR=52.90.51.177'  >> ~/.bashrc
		echo 'export CASSANDRA_CLIENT_ADDR=54.164.75.16'  >> ~/.bashrc
		echo 'export CASSANDRA_CLIENT_ADDR=52.87.229.186' >> ~/.bashrc

	On client:
		echo 'export CASSANDRA_SERVER_ADDR=23.22.13.183'   >> ~/.bashrc
		echo 'export CASSANDRA_SERVER_ADDR=23.22.38.77'    >> ~/.bashrc
		echo 'export CASSANDRA_SERVER_ADDR=54.197.159.32'  >> ~/.bashrc
		echo 'export CASSANDRA_SERVER_ADDR=184.73.58.105'  >> ~/.bashrc
		echo 'export CASSANDRA_SERVER_ADDR=54.161.196.221' >> ~/.bashrc

	exit and login back to make them take effect


Setup Cassandra directory both on server and client. It goes to the root
volume. It's okay. Can be done in the background to save time.

	On server
		screen

		cd ~/work \
		&& git clone git@github.com:hobinyoon/apache-cassandra-2.2.3-src.git \
		&& ln -s ~/work/apache-cassandra-2.2.3-src ~/work/cassandra \
		&& cdcass \
		&& time ant

	On client
		screen

		cd ~/work \
		&& git clone git@github.com:hobinyoon/apache-cassandra-2.2.3-src.git \
		&& ln -s ~/work/apache-cassandra-2.2.3-src ~/work/cassandra \
		&& cd ~/work/cassandra/mtdb/loadgen \
		&& ./loadgen


Prepare a local SSD volume and an EBS volume
	lsblk
		NAME    MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT
		xvda    202:0    0     8G  0 disk
		└─xvda1 202:1    0     8G  0 part /
		xvdb    202:16   0  15.3G  0 disk /mnt
		xvdc    202:32   0  15.3G  0 disk
		xvdd    202:48   0     8G  0 disk

	sudo mkfs.ext4 -m 0 /dev/xvdd
	(xvdb is already formatted)

	sudo vi /etc/fstab
		/dev/xvdb /mnt/local-ssd  auto  defaults,nobootwait,comment=cloudconfig 0 2
		/dev/xvdf /mnt/ebs-mag  auto  defaults,nobootwait,comment=cloudconfig 0 2
		(EBS-SSD-GP2 is already mounted at /)

	On the machine with local-ssd and ebs-ssd-gp2:
		sudo umount /mnt \
		&& sudo mkdir -p /mnt/local-ssd \
		&& sudo mount /mnt/local-ssd \
		&& sudo chown -R ubuntu /mnt/local-ssd \
		&& mkdir /mnt/local-ssd/cass-data \
		&& mkdir ~/cass-data-vol \
		&& sudo ln -s ~/cass-data-vol /mnt/ebs-ssd-gp2 \
		&& mkdir /mnt/ebs-ssd-gp2/cass-data

	Add EBS mag
		sudo mkdir -p /mnt/ebs-mag \
		&& sudo mount /mnt/ebs-mag \
		&& sudo mkdir /mnt/ebs-mag/cass-data \
		&& sudo chown -R ubuntu /mnt/ebs-mag \
		&& sudo chown -R ubuntu /mnt/ebs-mag/cass-data


Run Mutants server
	Setup cold storage
		sudo mkdir -p /mnt/ebs-ssd-gp2/mtdb-cold \
		&& (sudo mkdir -p /mnt/ebs-mag/mtdb-cold || true) \
		&& sudo ln -s /mnt/ebs-ssd-gp2 /mnt/cold-storage \
		&& sudo chown -R ubuntu /mnt/ebs-ssd-gp2 \
		&& sudo chown -R ubuntu /mnt/cold-storage \
		&& sudo chown -R ubuntu /mnt/cold-storage/mtdb-cold \
		&& mkdir -p ~/work/cassandra/mtdb/logs/sar

	Setup data directory
		ln -s /mnt/local-ssd/cass-data ~/work/cassandra/data

	The above 4 steps at once. TODO: better make it as a script that can be run inside the VM.
		sudo umount /mnt \
		&& sudo mkdir -p /mnt/local-ssd \
		&& sudo mount /mnt/local-ssd \
		&& sudo chown -R ubuntu /mnt/local-ssd \
		&& mkdir /mnt/local-ssd/cass-data \
		&& mkdir ~/cass-data-vol \
		&& sudo ln -s ~/cass-data-vol /mnt/ebs-ssd-gp2 \
		&& mkdir /mnt/ebs-ssd-gp2/cass-data \
		&& sudo mkdir -p /mnt/ebs-mag \
		&& sudo mount /mnt/ebs-mag \
		&& sudo mkdir /mnt/ebs-mag/cass-data \
		&& sudo chown -R ubuntu /mnt/ebs-mag \
		&& sudo chown -R ubuntu /mnt/ebs-mag/cass-data \
		&& sudo mkdir -p /mnt/ebs-ssd-gp2/mtdb-cold \
		&& (sudo mkdir -p /mnt/ebs-mag/mtdb-cold || true) \
		&& sudo ln -s /mnt/ebs-mag /mnt/cold-storage \
		&& sudo chown -R ubuntu /mnt/ebs-ssd-gp2 \
		&& sudo chown -R ubuntu /mnt/cold-storage \
		&& sudo chown -R ubuntu /mnt/cold-storage/mtdb-cold \
		&& mkdir -p ~/work/cassandra/mtdb/logs/sar \
		&& ln -s /mnt/local-ssd/cass-data ~/work/cassandra/data

	Run server after dropping page cache
		screen

		sudo -- sh -c 'echo 1 > /proc/sys/vm/drop_caches' \
		&& cdcass \
		&& time ant \
		&& rm -rf ~/work/cassandra/data/* \
		&& (killall sar > /dev/null 2>&1 || true) \
		&& ((sar 1 > ~/work/cassandra/mtdb/logs/sar/sar-`date +'%y%m%d-%H%M%S'`) &) \
		&& bin/cassandra -f | grep --color -E '^|MTDB:'

	Watch free memory
	  watch -n 0.5 "free -mt"

	Pressure memory. It can be launched in the background, and monitored by htop.
	After Cassandra is ready.
	Watch sstables

		cd ~/work/cassandra/mtdb/tools/pressure-memory \
		&& mkdir -p .build \
		&& cd .build \
		&& cmake .. \
		&& make -j && (./pressure-memory &) \
		&& watchsstables

	Save screen layout
		TODO: can this be automated?
		Ctrl-a :layout save default


Run loadgen client, preferrably on a different machine
	It read the env var CASSANDRA_SERVER_ADDR.

	cd ~/work/cassandra/mtdb/loadgen \
	&& ./create-db.sh \
	&& ./loadgen

	Monitor it's not running behind.


Get loadgen client log to server machine. Better do this on the server node,
which is likely to have bigger log files. Process the experiment.

	(killall pressure-memory > /dev/null 2>&1 || true) \
	&& rsync -av $CASSANDRA_CLIENT_ADDR:work/cassandra/mtdb/logs/loadgen ~/work/cassandra/mtdb/logs \
	&& cd ~/work/cassandra/mtdb/process-log/calc-cost-latency-plot-tablet-timeline \
	&& (\rm *.pdf || true) \
	&& ./plot-cost-latency-tablet-timelines.py \
	&& du -hs ~/work/cassandra/data/ \
	&& scp -P 20022 *.pdf hobin@localhost:

Check Cassandra data size
	du -hs ~/work/cassandra/data/ \
	&& du -hs /mnt/cold-storage/mtdb-cold/


Copy logs to mts7
	s0
		rsync -av ubuntu@54.205.51.252:work/cassandra/mtdb/logs ~/work/cassandra/mtdb/


NFS
	server
		sudo vi /etc/exports
			/nfs-export       54.205.183.58/32(rw,fsid=0,insecure,no_subtree_check,async)

		sudo apt-get install nfs-kernel-server -y \
		&& sudo mkdir -p /nfs-export \
		&& sudo chown -R ubuntu /nfs-export \
		&& sudo service nfs-kernel-server restart

	client
		sudo vi /etc/fstab
			54.165.74.184:/   /mnt/nfs-ebs-ssd   nfs    auto  0  0

		sudo apt-get install nfs-common -y \
		&& sudo mkdir /mnt/nfs-ebs-ssd \
		&& sudo mount /mnt/nfs-ebs-ssd
